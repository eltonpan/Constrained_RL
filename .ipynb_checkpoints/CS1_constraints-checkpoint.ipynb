{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: \n",
    "# Elton Pan, Antonio del Rio Chanona\n",
    "\n",
    "import pylab\n",
    "import scipy.integrate as scp\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pylab import *\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import collections\n",
    "import numpy.random as rnd\n",
    "from scipy.spatial.distance import cdist\n",
    "import sobol_seq\n",
    "from scipy.optimize import minimize\n",
    "eps  = np.finfo(float).eps\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "matplotlib.rcParams['font.sans-serif'] = \"Helvetica\"\n",
    "matplotlib.rcParams['font.family'] = \"Helvetica\"\n",
    "matplotlib.rcParams['font.size'] = 20\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from IPython.display import Audio # Import sound alert dependencies\n",
    "from IPython import display # For live plots\n",
    "def Done():\n",
    "    display(Audio(url='https://sound.peal.io/ps/audios/000/000/537/original/woo_vu_luvub_dub_dub.wav', autoplay=True))\n",
    "# Insert whatever audio file you want above\n",
    "\n",
    "############ Defining Environment ##############\n",
    "\n",
    "class Model_env: \n",
    "    \n",
    "    # --- initializing model --- #\n",
    "    def __init__(self, parameters, tf):\n",
    "        \n",
    "        # Object variable definitions\n",
    "        self.parameters       = parameters\n",
    "        self.tf = tf  \n",
    "        \n",
    "    # --- dynamic model definition --- #    \n",
    "    # model takes state and action of previous time step and integrates -- definition of ODE system at time, t\n",
    "    def model(self, t, state):\n",
    "        # internal definitions\n",
    "        params = self.parameters\n",
    "        u_L  = self.u0[0] # Control for light intensity\n",
    "        u_Fn = self.u0[1] # Control for nitrate input\n",
    "                \n",
    "        # state vector # Added Cq as the third state element\n",
    "        Cx  = state[0]\n",
    "        Cn  = state[1]\n",
    "        Cq  = state[2]\n",
    "        \n",
    "        # parameters # Updated with new params\n",
    "        u_m = params['u_m']; k_s = params['k_s'];\n",
    "        k_i = params['k_i']; K_N = params['K_N'];\n",
    "        u_d = params['u_d']; Y_nx = params['Y_nx'];\n",
    "        k_m = params['k_m']; k_sq = params['k_sq'];\n",
    "        k_iq = params['k_iq']; k_d = params['k_d'];\n",
    "        K_Np = params['K_Np'];\n",
    "        \n",
    "        # algebraic equations\n",
    "        \n",
    "        # variable rate equations\n",
    "        dev_Cx = (u_m * u_L * Cx * Cn/(Cn+K_N))/(u_L+k_s+u_L**2/k_i) - u_d*Cx\n",
    "        dev_Cn = (- Y_nx*u_m* u_L * Cx * Cn/(Cn+K_N))/(u_L+k_s+u_L**2/k_i) + u_Fn\n",
    "        dev_Cq = (k_m * u_L * Cx)/(u_L+k_sq+u_L**2/k_iq) - k_d * Cq/(Cn+K_Np)\n",
    "\n",
    "        return np.array([dev_Cx, dev_Cn, dev_Cq],dtype='float64') # Added Cq\n",
    "\n",
    "    def simulation(self, x0, controls):\n",
    "        # internal definitions\n",
    "        model, tf     = self.model, self.tf\n",
    "        self.controls = controls\n",
    "        \n",
    "        # initialize simulation\n",
    "        current_state = x0\n",
    "        \n",
    "        # simulation #ONLY ONE STEP unlike the previous code shown above\n",
    "        self.u0   = controls\n",
    "        ode       = scp.ode(model)                      # define ode\n",
    "        ode.set_integrator('lsoda', nsteps=3000)        # define integrator\n",
    "        ode.set_initial_value(current_state, tf)         # set initial value\n",
    "        current_state = list(ode.integrate(ode.t + tf)) # integrate system\n",
    "        xt            = current_state                   # add current state Note: here we can add randomnes as: + RandomNormal noise\n",
    "        \n",
    "        return xt\n",
    "\n",
    "    def MDP_simulation(self, x0, controls): #simulate ONLY ONE STEP\n",
    "        xt          = self.simulation(x0, controls) #simulate\n",
    "####         xt_discrete = self.discrete_env(xt) # make output state discrete\n",
    "####         return xt_discrete\n",
    "        return xt #remove this if you want to discretize\n",
    "\n",
    "    def reward(self, state):\n",
    "        reward = 100*state[-1][0] - state[-1][1]              # objective function 1\n",
    "        return reward\n",
    "\n",
    "p = {'u_m' : 0.0923*0.62, 'k_s' : 178.85, 'k_i' : 447.12, 'K_N' : 393.10,\n",
    "'u_d' : 0.001, 'Y_nx' : 504.49, 'k_m' : 2.544*0.62*1e-4,  'k_sq' : 23.51,\n",
    "'k_iq' : 800.0, 'k_d' : 0.281, 'K_Np' : 16.89}\n",
    "\n",
    "tf       = 240./12. # assuming 12 steps, we divide the whole horizon (240 h) over 12 for one step\n",
    "\n",
    "\n",
    "# Creating the model\n",
    "MDP_BioEnv = Model_env(p, tf)\n",
    "\n",
    "\n",
    "def transition(old_state, action):\n",
    "    '''Gives the new state given the current state and action\n",
    "       Arguments\n",
    "       old state : [Cx, Cn, Cq, t] \n",
    "       action    : [u_L, u_Fn]\n",
    "    \n",
    "       Output \n",
    "       new state : [Cx, Cn, Cq, t]\n",
    "       reward    : [Cq_Terminal]\n",
    "       '''\n",
    "    # If terminal state is reached (episode in final step)\n",
    "    if abs(old_state[3] - 240.) < 0.1:\n",
    "        reward    = old_state[2]           # Reward at terminal state is concentration of q\n",
    "        new_state = MDP_BioEnv.MDP_simulation(old_state[:-1], action)  # Take action and evolve using model\n",
    "        new_state.append(old_state[-1] + tf)                           # Increment time by tf (20 h)\n",
    "    \n",
    "    # Else if past terminal state (episode has ended)\n",
    "    elif (old_state[3] > 240.):\n",
    "        reward    = 0         # Zero reward given\n",
    "        new_state = old_state # Loop back to itself (no evolution)\n",
    "    \n",
    "    # Else non-terminal state (episode has not ended)\n",
    "    else:\n",
    "        reward    = 0         # Zero reward given\n",
    "        new_state = MDP_BioEnv.MDP_simulation(old_state[:-1], action) # Take action and evolve using model\n",
    "        new_state.append(old_state[-1] + tf)                          # Increment time by tf (20 h)\n",
    "\n",
    "    return new_state, reward\n",
    "\n",
    "def generate_random_episode(initial_state): \n",
    "    '''Generates an episode [\n",
    "                             [ [Cx, Cn, Cq, t], [u_L, u_Fn], reward ]\n",
    "                             ...\n",
    "                             ...\n",
    "                             ]\n",
    "    with random policy with\n",
    "    an initial state\n",
    "    '''\n",
    "    # Initial state\n",
    "    state  = initial_state # initial state\n",
    "    episode = []\n",
    "    \n",
    "    # Simulate 13 steps (not 12 because we want to exit the terminal step)\n",
    "    for step in range(13):\n",
    "        old_state = state                  # Save old state\n",
    "        u_L  = np.random.uniform(120, 400) # Pick random u_L\n",
    "        u_Fn = np.random.uniform(0, 40)    # Pick random u_Fn\n",
    "        action = [u_L, u_Fn]\n",
    "        \n",
    "        state, reward = transition(state, action)\n",
    "        episode       += [[old_state, action, reward]]\n",
    "    return episode\n",
    "generate_random_episode(initial_state = [1.0,150.0,0,0]) # Test generate_random_episode\n",
    "\n",
    "\n",
    "def extract_data_from_episode(episode, discount_factor = 0.9):\n",
    "    '''\n",
    "    Argument: An episode generated using the generate_random_episode() function\n",
    "    \n",
    "    Output: 13 Datapoints in the form of [[Cx, Cn, Cq, t, u_L, u_Fn], Q] for training \n",
    "            the Q-network\n",
    "    '''\n",
    "    Q_data = []\n",
    "    for step in reversed(range(13)): # Each episode has 13 entries, and Q table is updated in reversed order\n",
    "        state, action, reward = episode[step] \n",
    "\n",
    "        if step == 12: # If terminal state i.e. t = 240\n",
    "            G = reward # Return = reward at terminal state\n",
    "        else:\n",
    "            G = reward + discount_factor * G  # Return = reward + discounted return of the PREVIOUS state\n",
    "        \n",
    "        u_L, u_Fn = action     #  Unpack controls\n",
    "        state.append(u_L)       # Append u_L\n",
    "        state.append(u_Fn)      # Append u_Fn\n",
    "        data_point = [state, G] # Construct datapoint where state is index 0, and return is index 1\n",
    "        Q_data += [data_point]\n",
    "    return Q_data\n",
    "# episode = generate_random_episode(initial_state = [1.0,150.0,0,0])\n",
    "# extract_data_from_episode(episode, discount_factor = 0.9) # Test function\n",
    "\n",
    "def standardize_state_Q(state):\n",
    "    '''Argument: Un-standardized [Cx, Cn, Cq, t, u_L, u_Fn]\n",
    "    Output: Standardized [Cx, Cn, Cq, t, u_L, u_Fn] using previously determined mean and std values''' \n",
    "    for feature in range(len(x_mean_Q)): \n",
    "        state[feature] = (state[feature] - x_mean_Q[feature])/x_std_Q[feature]\n",
    "    return state\n",
    "\n",
    "def take_random_action(epsilon): # Epsilon represents the probability of taking a random action\n",
    "    if np.random.uniform(0,1) < epsilon:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def max_action(state):\n",
    "    '''Argument: State   [Cx, Cn, Cq, t]\n",
    "       Output  : Control [u_L, u_Fn] that maximizes Q_value using stochastic optimization'''\n",
    "    \n",
    "    # ROUND ONE: Get a ROUGH estimate of max action\n",
    "    action_distribution = []\n",
    "    for i in range(200): # Take 100 actions\n",
    "        u_L  = np.random.uniform(120, 400) # Pick random u_L\n",
    "        u_Fn = np.random.uniform(0, 40)    # Pick random u_Fn\n",
    "        action_distribution += [[u_L, u_Fn]]\n",
    "\n",
    "    inputs = []\n",
    "    for a in action_distribution:\n",
    "        s = state.copy()\n",
    "        s.append(a[0]) # Append u_L\n",
    "        s.append(a[1]) # Append u_Fn\n",
    "        s = standardize_state_Q(s) # Standardize the input using function defined\n",
    "        inputs += [s]\n",
    "    inputs          = torch.tensor(inputs)\n",
    "    index_of_highest_Q = np.argmax(Q_net(inputs).detach().numpy()) \n",
    "\n",
    "    # Unstandardize controls\n",
    "    max_u_L  = (inputs[index_of_highest_Q][4] * x_std_Q[4]).item() + x_mean_Q[4]\n",
    "    max_u_Fn = (inputs[index_of_highest_Q][5] * x_std_Q[5]).item() + x_mean_Q[5]\n",
    "    max_control = [max_u_L, max_u_Fn]\n",
    "\n",
    "    # ROUND TWO: Get a PRECISE estimate of max action\n",
    "    action_distribution = []\n",
    "    for i in range(200): # Take 100 actions\n",
    "        u_L  = np.random.uniform(max(max_control[0]-50, 120), min(max_control[0]+50, 400))\n",
    "        u_Fn = np.random.uniform(max(max_control[1]-5, 0), min(max_control[1]+5, 40))\n",
    "        action_distribution += [[u_L, u_Fn]]\n",
    "\n",
    "    inputs = []\n",
    "    for a in action_distribution:\n",
    "        s = state.copy()\n",
    "        s.append(a[0]) # Append u_L\n",
    "        s.append(a[1]) # Append u_Fn\n",
    "        s = standardize_state_Q(s) # Standardize the input using function defined\n",
    "        inputs += [s]\n",
    "\n",
    "    inputs          = torch.tensor(inputs)\n",
    "    index_of_highest_Q = np.argmax(Q_net(inputs).detach().numpy()) \n",
    "\n",
    "    # Unstandardize controls\n",
    "    max_u_L  = (inputs[index_of_highest_Q][4] * x_std_Q[4]).item() + x_mean_Q[4]\n",
    "    max_u_Fn = (inputs[index_of_highest_Q][5] * x_std_Q[5]).item() + x_mean_Q[5]\n",
    "    max_control = [max_u_L, max_u_Fn] # Predict action with the highest Q value\n",
    "\n",
    "    return max_control\n",
    "# max_action([1.0, 150.0, 0, 0]) # Test function\n",
    "\n",
    "def score_NN_policy(NN, initial_state, num_iterations = 10, get_control = False):\n",
    "    total_score = 0\n",
    "    sum_of_policies = []\n",
    "    for j in range(num_iterations): \n",
    "        state     = initial_state.copy() # Initial state\n",
    "        x_data    = [state[0]] # Store initial data\n",
    "        N_data    = [state[1]]\n",
    "        q_data    = [state[2]]\n",
    "        t_data    = [state[3]]\n",
    "        my_policy = []\n",
    "        for i in range(12): #take ten steps\n",
    "            action     = NN(state) # Predict action using NN\n",
    "            state      = transition(state, action)[0]\n",
    "            my_policy += [action]\n",
    "            x_data    += [state[0]]\n",
    "            N_data    += [state[1]]\n",
    "            q_data    += [state[2]]\n",
    "            t_data    += [state[3]]\n",
    "        sum_of_policies += [my_policy] # Create list of lists\n",
    "        score            = q_data[12]\n",
    "        total_score     += score\n",
    "    if get_control == True:\n",
    "        return total_score/num_iterations, np.array(sum_of_policies)\n",
    "    else:\n",
    "        return total_score/num_iterations\n",
    "# score_NN_policy(max_action, [1.0, 150.0, 0, 0], num_iterations = 10)\n",
    "\n",
    "def generate_episode_with_NN(NN, initial_state, epsilon, threshold = 0, constraint = False): \n",
    "    '''Generates an episode with the chosen action of each step having:\n",
    "    Probability of epsilon       ---> random action\n",
    "    Probability of (1 - epsilon) ---> greedy action (according to neural network)\n",
    "    '''\n",
    "    episode = []\n",
    "    state = initial_state # Initial state\n",
    "\n",
    "    if not constraint: # IF CONSTRAINTS ARE SWITCHED OFF\n",
    "        for i in range(13): #take (12 + 1) steps\n",
    "            old_state = state # Old state for storing into episode\n",
    "\n",
    "            if take_random_action(epsilon): # Take random action\n",
    "                u_L  = np.random.uniform(120, 400) # Pick random u_L\n",
    "                u_Fn = np.random.uniform(0, 40)    # Pick random u_Fn\n",
    "                action = [u_L, u_Fn]\n",
    "\n",
    "            else:                           # Else take greedy action\n",
    "                action = max_action(old_state)\n",
    "\n",
    "            state, reward  = transition(state, action)     # Evolve to get new state \n",
    "            episode       += [[old_state, action, reward]] # Update step\n",
    "    return episode\n",
    "# generate_episode_with_NN(max_action, [1.0, 150.0, 0, 0], epsilon = 0.1, threshold = 0, constraint = False)\n",
    "\n",
    "def update_replay_buffer(replay_buffer, episode, discount_factor = 0.9):\n",
    "    '''Argument: replay buffer (a collections.deque object) and ONE episode\n",
    "    Output: Adds standardized datapoints [[Cx, Cn, Cq, t, u_L, u_Fn], Q] for training NN into replay buffer\n",
    "    '''\n",
    "    data = extract_data_from_episode(episode, discount_factor = discount_factor) # Extract data points from episode\n",
    "    for data_point in data:\n",
    "        X, y = data_point # Unpack datapoint\n",
    "        X = standardize_state_Q(X) # Standardize X\n",
    "        y = ((y - y_mean_Q)/y_std_Q)[0] # Standardize y\n",
    "        data_point = [X, y] # Repack datapoint\n",
    "        replay_buffer.extend([data_point]) # Add to replay buffer\n",
    "    \n",
    "    return replay_buffer\n",
    "# replay_buffer = collections.deque(maxlen = 3000) # Max capacity of 3000\n",
    "# episode = generate_episode_with_NN(max_action, [1.0, 150.0, 0, 0], epsilon = 0.1, threshold = 0, constraint = False)\n",
    "# update_replay_buffer(replay_buffer, episode)\n",
    "\n",
    "def plot_episode(episode):\n",
    "    '''Plots an episode and corresponding score'''\n",
    "    x_list = []\n",
    "    N_list =[]\n",
    "    q_list =[]\n",
    "    t_list =[]\n",
    "    u_L_list= []\n",
    "    u_Fn_list = []\n",
    "    reward_list = []\n",
    "    t = 0\n",
    "    for step in episode:\n",
    "        [Cx, Cn, Cq, t], [u_L, u_Fn], reward = step\n",
    "        x_list += [Cx]\n",
    "        N_list += [Cn]\n",
    "        q_list += [Cq]\n",
    "        t_list += [t]\n",
    "        u_L_list+= [u_L]\n",
    "        u_Fn_list += [u_Fn]\n",
    "        reward_list += [reward]\n",
    "        t += tf\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    plt.subplot(3,2,1)\n",
    "    plt.plot(t_list, x_list, label = 'x trajectory')\n",
    "    plt.legend(fontsize = 15)\n",
    "\n",
    "    plt.subplot(3,2,2)\n",
    "    plt.plot(t_list, N_list, label = 'N trajectory')\n",
    "    plt.legend(fontsize = 15)\n",
    "\n",
    "    plt.subplot(3,2,3)\n",
    "    plt.plot(t_list, q_list, label = 'q trajectory')\n",
    "    plt.legend(fontsize = 15)\n",
    "\n",
    "    plt.subplot(3,2,4)\n",
    "    plt.step(t_list, u_L_list, label = 'u_L trajectory')\n",
    "    plt.legend(fontsize = 15)\n",
    "\n",
    "    plt.subplot(3,2,5)\n",
    "    plt.step(t_list, u_Fn_list, label = 'u_Fn trajectory')\n",
    "    plt.legend(fontsize = 15)\n",
    "\n",
    "    plt.show()\n",
    "    print('Score:', q_list[-1])\n",
    "    # print('x_list:', x_list)\n",
    "    # print('n_list:', N_list)\n",
    "    # print('q_list:', q_list)\n",
    "    # print('reward_list:', reward_list)\n",
    "    # print('t_list:', t_list)\n",
    "# episode = generate_episode_with_NN(NN = max_action, initial_state = [1.0, 150.0, 0, 0], epsilon = 0, threshold = 0, constraint = False)\n",
    "# plot_episode(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_constraint_values_from_episode(episode, N_limit = 800, terminal_N_limit = 150):\n",
    "    '''Arguments : 1 episode and N_limit\n",
    "       Output    : Index 0 gives input data in terms of [Cx, Cn, Cq, t-t_f, u_L, u_Fn]\n",
    "                   Index 1 gives target constraint values'''\n",
    "\n",
    "    state_action = [] # Initialize inputs\n",
    "#     g1_target   = [] # Initialize target value (constraints)\n",
    "#     g2_target   = [] # Initialize target value (constraints)\n",
    "#     g3_target   = [] # Initialize target value (constraints)\n",
    "    targets = [] # Initialize target values [[g1, g2 g3], ...]\n",
    "    \n",
    "    num_transitions = 12\n",
    "    for i in range(num_transitions): # Index 0 to 11 instead of 12 because we consider the 1st 12 transitions\n",
    "        step             = episode[i]       # Choose a specific step\n",
    "        new_step         = episode[i+1]     # and the corresponding subsequent step\n",
    "        \n",
    "        state, action, _ = step             # Unpack state & action\n",
    "        state            = list(state)\n",
    "        state[3]         = 240 - state[3]   # IMPORTANT: Modify t to (t_f - t)\n",
    "        state           += action           # Append u_L and u_Fn\n",
    "        \n",
    "        new_state, _, _  = new_step         # Unpack subsequent state\n",
    "        Cx               = new_state[0]     # Biomass of SUBSEQUENT step (for g2)\n",
    "        Cn               = new_state[1]     # Nitrate of SUBSEQUENT step (for g1)\n",
    "        Cq               = new_state[2]     # Bioproduct of SUBSEQUENT step (for g2)\n",
    "        \n",
    "        if i == (num_transitions-1): # If final transition\n",
    "            Cn_T         = new_state[1]     # Nitrate of TERMINAL state (for g3)\n",
    "            \n",
    "        # INPUT for training\n",
    "        state_action += [state]\n",
    "\n",
    "        # TARGETS for path constraint 1 (g1) where Cn - 800 =< 0\n",
    "        g1           = Cn - N_limit # Calculate constraint value (+ve for exceeding limit)\n",
    "        g1_target   += [g1]       # TARGET OUTPUT (IMPORTANT TO USE A NESTED LIST hence the DOUBLE square brackets)\n",
    "\n",
    "        # TARGETS for path constraint 2 (g2) where Cq - 0.011*Cx =< 0\n",
    "        g2           = Cq - 0.011 * Cx # Calculate constraint value (+ve for exceeding limit)\n",
    "        g2_target   += [g2]       # TARGET OUTPUT (IMPORTANT TO USE A NESTED LIST hence the DOUBLE square brackets)\n",
    "        \n",
    "    # TARGETS for terminal constraint 2 (g3) where Cq - 0.011*Cx =< 0\n",
    "    g3           = Cn_T - terminal_N_limit # Calculate constraint value (+ve for exceeding limit)\n",
    "    g3_target   += [g3]*num_transitions # End constraint violation same for all steps in same episode\n",
    "    \n",
    "    # Update constraints using \"Crystal Ball/Oracle\": Highest/Worst constraint value from current and future steps\n",
    "    for j in range(num_transitions - 1): # Index of constraint value to be checked (11 instead of 12 bcos the 12th value is terminal and has no future)\n",
    "        for k in range(num_transitions-j): # Number of steps into the future\n",
    "            if g1_target[j] < g1_target[j+k]: # If future value is LARGER than current value\n",
    "                g1_target[j] = g1_target[j+k]  # Replace current value with future value\n",
    "            if g2_target[j] < g2_target[j+k]: # If future value is LARGER than current value\n",
    "                g2_target[j] = g2_target[j+k]  # Replace current value with future value\n",
    "    \n",
    "#     print('g1:', g1_target)\n",
    "#     print('')\n",
    "#     print('g2:', g2_target)\n",
    "#     print('')\n",
    "#     print('g3:', g3_target)  \n",
    "    g1_g2_zip = list(zip(g1_target,g2_target))\n",
    "    g1_g2_g3_zip = list(zip(g1_g2_zip,g3_target))\n",
    "    print(g1_g2_g3_zip)  \n",
    "#     return state_action, constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1.0, 150.0, 0, 0], [316.5655936007573, 33.46297046540691], 0],\n",
       " [[1.2612962896505129, 676.2377112350939, 0.0023692907147349353, 20.0],\n",
       "  [138.63436871289196, 21.461767529339056],\n",
       "  0],\n",
       " [[1.6580668207836153, 890.702031771864, 0.005742098650937933, 40.0],\n",
       "  [133.64777576563188, 36.00406541631423],\n",
       "  0],\n",
       " [[2.23686616311861, 1299.3318169167312, 0.010231651081886131, 60.0],\n",
       "  [125.41401071015767, 28.283502366753254],\n",
       "  0],\n",
       " [[3.0464367558264076, 1430.1531145232075, 0.016325781277107522, 80.0],\n",
       "  [186.8314949266457, 39.357381232461066],\n",
       "  0],\n",
       " [[4.372671457934025, 1511.2153904923257, 0.024763179104064496, 100.0],\n",
       "  [287.60933851745074, 30.700510001987446],\n",
       "  0],\n",
       " [[6.324329439180258, 1087.1416986079078, 0.036238325570838135, 120.0],\n",
       "  [153.43702370305624, 31.899311274228175],\n",
       "  0],\n",
       " [[8.443163004493933, 581.8266660654235, 0.05322087079867548, 140.0],\n",
       "  [315.3678120211129, 25.6663225449587],\n",
       "  0],\n",
       " [[10.169947954191592, 128.89844701204325, 0.07207282465292948, 160.0],\n",
       "  [294.0707730508893, 20.52019327984457],\n",
       "  0],\n",
       " [[10.889703960738283, 69.62869232415565, 0.09041998339636792, 180.0],\n",
       "  [208.59755831596152, 2.4785785857631915],\n",
       "  0],\n",
       " [[10.892621851005712, 7.41630036217776, 0.09774277631883439, 200.0],\n",
       "  [195.8220223104463, 38.28065752643581],\n",
       "  0],\n",
       " [[11.917818657753807, 141.58705806415202, 0.11885399055478246, 220.0],\n",
       "  [237.5575215145949, 28.901218062615936],\n",
       "  0],\n",
       " [[12.924492389280328, 86.12881159884533, 0.14062209019035793, 240.0],\n",
       "  [314.32432763655254, 34.01789694370127],\n",
       "  0.14062209019035793]]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode = generate_random_episode(initial_state = [1.0,150.0,0,0]) \n",
    "episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((711.2153904923257, -0.0015473260917256704), -63.87118840115467), ((711.2153904923257, -0.0015473260917256704), -63.87118840115467), ((711.2153904923257, -0.0015473260917256704), -63.87118840115467), ((711.2153904923257, -0.0015473260917256704), -63.87118840115467), ((711.2153904923257, -0.0015473260917256704), -63.87118840115467), ((287.14169860790776, -0.0015473260917256704), -63.87118840115467), ((-218.17333393457648, -0.0015473260917256704), -63.87118840115467), ((-658.412941935848, -0.0015473260917256704), -63.87118840115467), ((-658.412941935848, -0.0015473260917256704), -63.87118840115467), ((-658.412941935848, -0.0015473260917256704), -63.87118840115467), ((-658.412941935848, -0.0015473260917256704), -63.87118840115467), ((-713.8711884011547, -0.0015473260917256704), -63.87118840115467)]\n"
     ]
    }
   ],
   "source": [
    "extract_constraint_values_from_episode(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g1: [[384.7653509617776], [384.7653509617776], [384.7653509617776], [384.7653509617776], [384.7653509617776], [284.04073705193036], [-453.3271801091978], [-583.6305640320186], [-625.6026744504782], [-625.6026744504782], [-625.6026744504782], [-625.6026744504782]]\n",
      "\n",
      "g2: [[0.0031757331462454186], [0.0031757331462454186], [0.0031757331462454186], [0.0031757331462454186], [0.0031757331462454186], [0.0031757331462454186], [0.0031757331462454186], [0.0031757331462454186], [0.0031757331462454186], [0.0031757331462454186], [0.0031757331462454186], [0.0031757331462454186]]\n",
      "\n",
      "g3: [24.397325549521895, 24.397325549521895, 24.397325549521895, 24.397325549521895, 24.397325549521895, 24.397325549521895, 24.397325549521895, 24.397325549521895, 24.397325549521895, 24.397325549521895, 24.397325549521895, 24.397325549521895]\n"
     ]
    }
   ],
   "source": [
    "extract_constraint_values_from_episode(episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda0e6ce04c06ed4dc19a18eb97f4012dc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
